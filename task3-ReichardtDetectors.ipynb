{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"task3-ReichardtDetectors.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"19B1yyhGt2Sxw2PtQL93y7ulCPbfRRP2M","authorship_tag":"ABX9TyPgWK37eSSEZ3ik3v78vEJn"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"luWV5jrtXvjl"},"source":["# **Task 3 - Reichardt Detectors**\n","\n","<div>\n","    <img src=\"https://pbs.twimg.com/media/DV9-v1GW4AgTQoT.png\" width=190px; height=230px, align='right'/>\n","</div>\n","<!-- https://ars.els-cdn.com/content/image/1-s2.0-S0960982217300738-gr1.jpg -->\n","\n","This task will create an \"Hassenstein-Reichardt detector\", i.e. an elementary and hypothetical neural circuit inspired by the fruit fly. Such model is used to detect correlations between adjacent points in order to achieve motion perception. It consists of a cascade of spatial filter and temporal filter applied to adjacent locations on the input space. When an input (light, in case of visual systems) is received within a spatial kernel, a signal is sent and then delayed in time ($\\tau$). If the moving object in the scene has the correct (preferred) speed and direction, it will activate the adjacent spatial kernel after a time that equals the previous delay $\\tau$. The corresponding signal (with no delay) will then be summed to the delayed signal from the former location, eliciting an output in the final neuron. In case the object moves along the null direction (or with the wrong speed), the two signals will not be coincident and the final detector will thus not reach the firing threshold for emitting an output.\n","\n","In this implementation, we will use event-based data relative to a moving bar recorded from a neuromorphic sensor. We will not use the whole pixel array of the sensor (346x260) but the central squared region with shape 60x60 and only the ON polarity. The bar may either move rightward (R) or leftward (L) with 3 different speeds (v1, v2, v3). Code for the initial data loading and pre-processing is given. The exercise focuses on building the architecture of the network, which will consist of:\n","- a first input neural population with 60x60 neurons (pixels);\n","- a convolutional neural layer of 6 LIF neurons applying spatial filtering with a 2D elongated gaussian kernel;\n","- two output populations (with 1 neuron each) of motion sensitive LIF neurons taking delayed inputs (thus applying temporal filtering) from neurons in the convolutional layer.\n","\n","**Run the following cell before you start!**"]},{"cell_type":"code","metadata":{"id":"LtItfru-2xJy"},"source":["!pip3 install dv\n","!pip3 install brian2\n","from dv import AedatFile\n","from brian2 import *\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from matplotlib import rc\n","rc('animation', html='jshtml')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xqakrs4g1G53"},"source":["**In order to use the event-based data stored in the drive folder, you will have to give permissions to this notebook to access google drive.** To do so click the link that appears after you run the cell below. It will redirict you to a page where you will have to confirm your permission and copy the authorization code that appears. Paste the code below and press enter."]},{"cell_type":"code","metadata":{"id":"DBIN2nE54EJI"},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')\n","data_dir = \"drive/My Drive/Neuromorphic_Exe20-21/data\"\n","!cp \"drive/My Drive/Neuromorphic_Exe20-21/utils/DAVIS_numpy_utils.py\" .\n","!cp \"drive/My Drive/Neuromorphic_Exe20-21/utils/kernels.py\" .\n","from DAVIS_numpy_utils import *\n","from kernels import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jV7N3Vf63KMx"},"source":["Define some parameters of the event-based data you want to load. Particularly, you may choose between 2 directions and 3 speeds of motion."]},{"cell_type":"code","metadata":{"id":"Vz0Nlsnr4X-r"},"source":["# You may change these variables, related to the motion of the input stimulus:\n","direction = 'R'    # 'R' or 'L'\n","speed = 'v3'       # 'v1', 'v2' or 'v3'\n","\n","# Do not change these variables:\n","duration = 0.3 * 10 ** 6   # duration of the simulation (us)\n","N_x, N_y = 346, 260        # shape of the whole DAVIS pixel array\n","input_size = 60            # shape of the central squared region we will consider"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IJEy95-j7Ddt"},"source":["## Load & Process Data"]},{"cell_type":"markdown","metadata":{"id":"kCIj0zT_3iBg"},"source":["Load the event-based data file (with AEDAT4 extension) containing all outputs from the DAVIS sensor. Store DVS events in a numpy array."]},{"cell_type":"code","metadata":{"id":"TnOuKCiP7IrU"},"source":["file = data_dir + \"/bar_\" + direction + '_' + speed + \".aedat4\"\n","with AedatFile(file) as f:\n","    # Put all events in a structured numpy array\n","    events_struct = np.hstack([event_packet for event_packet in f['events'].numpy()])\n","    # Access information of events by type\n","    ts, x, y, pol = events_struct['timestamp'], events_struct['x'], events_struct['y'], events_struct['polarity']\n","    # Put info in a 4d-array with N rows (number of events) and 4 columns (ts, x, y, p)\n","    events = np.stack((ts, x, y, pol)).T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OfeNoceE4HSV"},"source":["Show the DVS recording and distinguish between ON and OFF events with colors (green and red respectively)."]},{"cell_type":"code","metadata":{"id":"_6nnxSKWx1cJ"},"source":["dvsvid_on, dvsvid_off = dvs_computevideofreq_onoff(dvs_flattenaddress(events, N_x, N_y), N_x, N_y, rate=50)\n","minval, maxval = min(dvsvid_on.min(), dvsvid_off.min()), max(dvsvid_on.max(), dvsvid_off.max())\n","frames_dvs = np.zeros((*dvsvid_on.shape, 3))  # RGB video\n","frames_dvs[:, :, :, 0] = np.uint8(np.interp(dvsvid_off, (minval, maxval), (0, 255)))\n","frames_dvs[:, :, :, 1] = np.uint8(np.interp(dvsvid_on, (minval, maxval), (0, 255)))\n","\n","fig = plt.figure(figsize=(5, 4))\n","plt.title('DVS video: ON=green, OFF=red')\n","plt.axis('off')\n","plot_dvs_frames = []\n","for frame in frames_dvs:\n","    dvs_image = plt.imshow(frame.astype(np.uint8), vmin=0, vmax=255, animated=True)\n","    plot_dvs_frames.append([dvs_image])\n","plt.close()\n","animation.ArtistAnimation(fig, plot_dvs_frames, interval=30, blit=True, repeat_delay=1000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fPp8WEYk4l1w"},"source":["Pre-process the event-based data (mainly, cut the pixel array, cut the recording in the period where the stimulus is present and select ON events only). Then store timestamps and pixel addresses of events in different variables. We will use those as input to the network through a \"*SpikeGeneratorGroup*\"."]},{"cell_type":"code","metadata":{"id":"pEnmoLzD9Zsb"},"source":["dvs = dvs_cut_pixelarray(events, input_size, N_x, N_y)  # cut the pixel array to a squared region\n","dvs_flat = dvs_flattenaddress(dvs, input_size, input_size)  # from (x,y) coordinate to flatten index\n","dvs_flat_on = dvs_select_polarity(dvs_flat, polarity='on')  # select only ON events and remove the others\n","start = dvs_compute_start(dvs_flat_on, input_size, input_size, duration)  # compute the moment when the stimulus starts to move in the central region\n","dvs_flat_on = dvs_cut_timewindow(dvs_flat_on, start, start + duration)  # remove all events occurring before 'start' and after 'start + duration'\n","dvs_flat_on = dvs_timereset(dvs_flat_on)  # reset timestamps so that first event has 0 timestamp\n","ts, id = dvs_return_id_and_ts(dvs_flat_on)  # separate timestamps and flattened-address of events' array\n","ts, id = dvs_refractory_filter(address=id, timestamp=ts, refractory=100)  # apply refractory filter for a 100 us refractory period"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"26ckKFro5xoj"},"source":["Show the remaining events."]},{"cell_type":"code","metadata":{"id":"N77kllzq9hx7"},"source":["dvsvid_on = dvs_computevideofreq(dvs_flat_on, input_size, input_size, rate=80)\n","minval, maxval = dvsvid_on.min(), dvsvid_on.max()\n","frames_dvs = np.zeros((*dvsvid_on.shape, 3))  # RGB video\n","frames_dvs[:, :, :, 1] = np.uint8(np.interp(dvsvid_on, (minval, maxval), (0, 255)))\n","\n","fig = plt.figure(figsize=(5, 4))\n","plt.title('DVS video: ON=green, OFF=red')\n","plt.axis('off')\n","plot_dvs_frames = []\n","for frame in frames_dvs:\n","    dvs_image = plt.imshow(frame.astype(np.uint8), vmin=0, vmax=255, animated=True)\n","    plot_dvs_frames.append([dvs_image])\n","plt.close()\n","animation.ArtistAnimation(fig, plot_dvs_frames, interval=80, blit=True, repeat_delay=1000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zxrVlhyk9qkJ"},"source":["## Define Convolution"]},{"cell_type":"markdown","metadata":{"id":"tG0HSmHf6XWE"},"source":["Given the kernel dimension, convolution information and the shape of the input space, compute the dimension that the convolutional layer (the neural population after spatial filtering) must have."]},{"cell_type":"code","metadata":{"id":"_OUVNuBF9wIp"},"source":["kernel_sigma = 4\n","kernel_elongation = 10\n","kernel_th = 0.5\n","stride = 10\n","kernel_size = compute_dim_gauss2d_elong(input_size, sigma=kernel_sigma, p=kernel_elongation, threshold=kernel_th)\n","output_size, error_conv = convolution_out_shape(input_size, kernel_size=kernel_size, stride=stride, return_error=True)\n","print('Information of convolution on input pixel array:\\n'\n","      ' - input shape of pixel array is {}\\n'\n","      ' - kernel dimension is {}\\n'\n","      ' - stride is {}\\n'\n","      ' - output shape of convolutional layer is {}\\n'\n","      ' - error in sliding kernel over input is {}'\n","      .format(input_size, kernel_size, stride, output_size, round(error_conv, 2)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DRxV_czjFiQF"},"source":["Visualise convolution: show the spatial kernel sliding on the input space."]},{"cell_type":"code","metadata":{"id":"rNGxH3ApBk-v"},"source":["all_kernels = np.zeros((output_size, input_size ** 2))\n","y_center = int(round(input_size / 2))\n","for x in range(output_size):\n","  x_center = x * stride + int(round((kernel_size - 1) / 2))\n","  target = np.ravel_multi_index((y_center, x_center), (input_size, input_size))\n","  # i.e. target =  x_center + y_center * input_size\n","  all_kernels[x, :] = gauss2d_elong(target, sigma=kernel_sigma, p=kernel_elongation, teta=90, space_dim=input_size, threshold=kernel_th)\n","all_kernels = all_kernels.reshape(output_size, input_size, input_size)\n","\n","fig = plt.figure(figsize=(5, 4))\n","plt.title('Sliding Spatial Kernel')\n","plt.axis('off')\n","plot_kernel = []\n","for kernel in all_kernels:\n","    kernel_image = plt.imshow(kernel * 255, vmin=0, vmax=255, animated=True)\n","    plot_kernel.append([kernel_image])\n","plt.close()\n","animation.ArtistAnimation(fig, plot_kernel, interval=500, blit=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mtef4encQB9j"},"source":["## Build Network Architecture & Run Simulation"]},{"cell_type":"markdown","metadata":{"id":"LDp-3tEfbzrK"},"source":["Setup the simulation to use 0.1ms timesteps."]},{"cell_type":"code","metadata":{"id":"pC8WRA10VNLl"},"source":["# Write your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qbHYvqooORc3"},"source":["Create the input neural population (hint: use a \"*SpikeGeneratorGroup*\"). Neurons *id* must spike at times *ts*."]},{"cell_type":"code","metadata":{"id":"dNVhx6hmQYSh"},"source":["# Input layer (events from the sensor)\n","Layer_input = # ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Dz7jd1IOoPR"},"source":["Define the LIF neuron model. The resting value is -65mV, the threshold -50mV, the membrane time constant 2ms and the synapse time constant 5ms. Then create convolutional layer with a number of neurons equal to *ouput_size*. The reset potential is equal to the resting value and the refractory period is 150ms. Remember to initialize the membrane potential to the resting value."]},{"cell_type":"code","metadata":{"id":"QdyqDFccQXPV"},"source":["# Define neuron model and parameters\n","model = # ...\n","\n","# Convolutional layer\n","Layer_convolution = # ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JyU9xhXYPnrp"},"source":["Create the two output population with 1 LIF neuron each. The reset potential is equal to the resting value and the refractory period is 50ms."]},{"cell_type":"code","metadata":{"id":"TbBHm5BSQWSW"},"source":["# RIGHTWARD Reichardt detector neuron\n","Layer_reichardt_R = # ...\n","\n","# LEFTWARD Reichardt detector neuron\n","Layer_reichardt_L = # ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"13h7J0KyOgA6"},"source":["Create the convolutional synapses, which define the spatial filtering between the input population and the convolutional layer. The spatial kernel (shown in the animation above) should shift only along the x dimension of the input space by a number of pixels defined by the variable *stride*. The variables *x_center* and *y_center* define the coordinates of the central pixel of the kernel."]},{"cell_type":"code","metadata":{"id":"79NpHLCyQuye"},"source":["# Synapses: convolution\n","Syn_convolution = # ...\n","\n","y_center = # ...\n","\n","kernel_weights = np.asarray([])\n","for x in range(output_size):\n","\n","    x_center = # ...\n","\n","    target_pixel = x_center + y_center * input_size\n","    kernel = gauss2d_elong(target_pixel, sigma=kernel_sigma, p=kernel_elongation, teta=90, space_dim=input_size, threshold=kernel_th)\n","    indices_kernel = np.where(kernel >= kernel_th)[0]\n","    Syn_convolution.connect(i=indices_kernel, j=x)\n","    kernel_weights = np.concatenate((kernel_weights, kernel[indices_kernel]))\n","Syn_convolution.w = kernel_weights * mV"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cEm8RGKLRevw"},"source":["Create 2 delayed synapses groups (defining the temporal filtering) between the convolution layer and the 2 output (motion sensitive) neurons. Connectivity should be \"all-to-all\" and all weights should be equal to 10mV. The delay between adjacent spatial filters should be equal to *delay_reichardt*. In case of rightward sensitivity, the delay should decrease from the first synapse (connecting the first neuron of the convolutional population with the R neuron) to the last (connecting the last neuron of the convolutional population with the R neuron): i.e. the last synapse should have 0 delay, the second *delay_reichardt* and so on. In case of leftward sensitivity, the delay should increase from the first synapse to the last one. Note that, as we fixed the spatial filtering, the delay is what defines the preferred speed of the neurons and must therefore be adapted to the speed of the bar."]},{"cell_type":"code","metadata":{"id":"DJFvxxArSTjR"},"source":["# Adapt the delay to the speed of the input stimulus \n","delay_reichardt = {'v1': 6, 'v2': 30, 'v3': 50}.get(speed) * ms\n","\n","# Synapses: reichardt model - RIGHTWARD motion selectivity\n","Syn_reichardt_R = # ...\n","\n","# Synapses: reichardt model - LEFTWARD motion selectivity\n","Syn_reichardt_L = # ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M-Tlm7WEOvaf"},"source":["Monitor membrane potentials and spikes from the convolutional layer and both output neurons."]},{"cell_type":"code","metadata":{"id":"kG32zBNRck40"},"source":["# Write your code here "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F_jS-vrAOzMl"},"source":["Build and feed the network object."]},{"cell_type":"code","metadata":{"id":"sKNytsGBdGw0"},"source":["# Write your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"liRUeC68PHBB"},"source":["Run the simulation for 300ms."]},{"cell_type":"code","metadata":{"id":"twmG6uIRdHdW"},"source":["# Write your code here "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A8stQSwFPWST"},"source":["Plot the membrane potentials of all neurons in the convolutional layer and show a rasterplot of all spikes in such layer."]},{"cell_type":"code","metadata":{"id":"NsBXNlBffi5D"},"source":["# Write your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cx-jE2VsebWV"},"source":["**From the rasterplot, you should notice that the moving bar defines an orientation in space-time, where the sign of the slope is related to the direction of the motion and its magnitude is related to the speed.**"]},{"cell_type":"markdown","metadata":{"id":"WKNEKGKvfZnQ"},"source":["Print the time interval between spikes in consecutive neurons of the convolutional layer. If the preferred speed of the output neurons matches the actual speed of the input stimulus, such delays should all be very close to the one that we have set between adjacent spatial filters."]},{"cell_type":"code","metadata":{"id":"piKj5iL-fgxE"},"source":["# Write your code here "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x3GvE8wchMU9"},"source":["Plot the membrane potentials of the 2 output neurons and show a rasterplot of their spiking activity."]},{"cell_type":"code","metadata":{"id":"B24DaCRMhMeH"},"source":["# Write your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ITuUuJth337"},"source":["**Only the neuron that is sensitive to the same direction of motion as the input bar should fire a spike. In this case, the input spikes from the convolutional layer will be coincident (thanks to the correct temporal filterings) thus summing up and triggering the target neuron. Instead, the inputs of the neuron sensitive to the opposite direction will not be coincident and this detector will thus not reach the firing threshold for emitting a spike.**"]},{"cell_type":"markdown","metadata":{"id":"-_cbmvqpGjwl"},"source":["\n","---\n","\n","#### **Extensions**\n","\n","1. Try to change the direction and speed of the input data and watch the resulting behavior of the network.\n","2. Try to add some other output neurons sensitive to different speeds of motion to view their response in case of non-preferred input speeds. Particularly, if you set the delay for the first pair of output neurons (R and L) so that their sensitivity is to the speed *v1*, you may add 2 more pairs of neurons sensitive to the speeds *v2* and *v3* respectively. Then change speed and direction of the stimulus and observe the activity of all such neurons. Note: this task is left to the most willing students and the solution will <u>not</u> be given."]}]}
